# End-to-End Data Project Setup

## ğŸ“ Projectstructuur

```
/mijn_project
â”‚â”€â”€ /data                # Ruwe en verwerkte data
â”‚   â””â”€â”€ database.db      # SQLite database
â”‚â”€â”€ /notebooks           # Jupyter Notebooks voor analyses
â”‚â”€â”€ /scripts             # ETL-scripts
â”‚   â””â”€â”€ etl.py
â”‚â”€â”€ /sql                 # SQL-queryâ€™s, tabellenstructuur
â”‚â”€â”€ /config              # Config (API-keys in .env)
â”‚   â””â”€â”€ .env
â”‚â”€â”€ /tests               # Unit tests
â”‚â”€â”€ main.py              # Hoofdscript om ETL te runnen
â”‚â”€â”€ requirements.txt     # Python dependencies
â”‚â”€â”€ README.md            # Project documentatie
â”‚â”€â”€ .gitignore           # Beschermt gevoelige files
```

---

## âœ… `.gitignore`

```
config/.env
data/*.db
__pycache__/
.ipynb_checkpoints/
```

---

## ğŸ” `.env` (voorbeeld)

```
API_URL="https://api.partnercenter.microsoft.com/v1/invoices"
API_KEY="YOUR_KEY_HERE"
```

---

## ğŸ“¦ `requirements.txt`

```
requests
pandas
python-dotenv
sqlite3-binary
```

---

## ğŸ” ETL Script â€“ `scripts/etl.py`

```python
import os
import requests
import pandas as pd
import sqlite3
from dotenv import load_dotenv

load_dotenv("config/.env")

API_URL = os.getenv("API_URL")
DB_PATH = "data/database.db"

def extract_data():
    headers = {"Authorization": f"Bearer {os.getenv('API_KEY')}"}
    r = requests.get(API_URL, headers=headers)
    r.raise_for_status()
    return r.json()

def transform_data(data):
    df = pd.DataFrame(data)
    df.columns = [c.strip().lower().replace(" ", "_") for c in df.columns]
    if "timestamp" in df.columns:
        df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")
    return df

def load_data(df):
    os.makedirs("data", exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    df.to_sql("facturen", conn, if_exists="replace", index=False)
    conn.close()
    print("âœ… Data loaded into SQLite!")

def run():
    data = extract_data()
    df = transform_data(data)
    load_data(df)

if __name__ == "__main__":
    run()
```

---

## â–¶ï¸ Hoofdscript â€“ `main.py`

```python
from scripts.etl import run

if __name__ == "__main__":
    print("ğŸš€ Starting ETL workflow...")
    run()
    print("ğŸ¯ Done.")
```

---

## ğŸ§ª Testvoorbeeld â€“ `tests/test_etl.py`

```python
from scripts.etl import transform_data

def test_transform():
    sample = [{"timestamp": "2025-01-01T10:00:00Z"}]
    df = transform_data(sample)
    assert "timestamp" in df.columns
    assert df["timestamp"].notnull().all()
```

---

## ğŸ§  SQL-voorbeeld â€“ `sql/check_facts.sql`

```sql
SELECT COUNT(*) AS aantal_records
FROM facturen;
```

---

## ğŸ““ Notebook-voorbeeld â€“ `notebooks/analyses.ipynb`

```python
import pandas as pd
df = pd.read_sql("SELECT * FROM facturen", "sqlite:///../data/database.db")
df.head()
```

---

## ğŸš€ Eerste run

```bash
pip install -r requirements.txt
python main.py
```

---

## ğŸ“ README.md (template)

```
# Mijn Data Project

Een end-to-end ETL-workflow:
â€¢ Extract: Data uit externe API
â€¢ Transform: Pandas schoonmaak
â€¢ Load: SQLite database

## Starten
pip install -r requirements.txt
python main.py
```

---

## ğŸ”„ Mogelijke uitbreidingen

- Logging (structuur i.p.v. print)
- Dagelijkse scheduler (cron / GitHub Actions)
- Upgrade van SQLite â†’ PostgreSQL of Fabric SQL endpoint
- Dashboard in Power BI / Fabric
- Versiebeheer van data met Delta Lake

---

## â„¹ï¸ Hoe gebruik je dit project? (+ voorbeelden)
- **Eerste keer opzetten**: `pip install -r requirements.txt` â†’ dependencies.  
- **Run ETL**: `python main.py` â†’ haalt API-data op en laadt in SQLite (`data/database.db`).  
- **Test transformer**: `pytest tests/test_etl.py -q` â†’ checkt dat `timestamp` parsed wordt.  
- **Check data** (shell): `sqlite3 data/database.db "SELECT COUNT(*) FROM facturen;"` â†’ snelle validatie.  
- **Aanpassen bron-URL**: wijzig `API_URL` in `.env`; herstart `main.py`.  
- **Schema aanpassen**: pas `transform_data` aan (kolommen hernoemen/typen) â†’ rerun ETL.  
```
